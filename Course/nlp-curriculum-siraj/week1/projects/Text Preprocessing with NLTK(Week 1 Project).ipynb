{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import pathlib\n",
    "import os\n",
    "import json\n",
    "#from stemming.porter2 import stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = str(pathlib.Path(os.path.abspath('')).parents[1])+'/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_texts = data_folder+\"test_doc.txt\"\n",
    "\n",
    "data = []\n",
    "# count each line individually\n",
    "with open(source_texts, 'r') as file:\n",
    "    # the indx of that line and the contents of that line\n",
    "    for (indx, line) in enumerate(file):\n",
    "        data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove new line from the list and \\n from the sentence\n",
    "data = [d.replace('\\n', '') for d in data if d != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization means\n",
    "\n",
    "changing to the same form like to lower case\n",
    "\n",
    "changing some shortform into their original word\n",
    "\n",
    "e.g is't --> is not\n",
    "   \n",
    "Tokenize means just changin a given sentence into a list of independent tokens or words as much as possible.\n",
    "\n",
    "but the greatest issue with Tokenization is its really difficult to tokenize into independent tokens.\n",
    "\n",
    "because of name, country name e.g Addis Ababa becomes two tokens but should have one token,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding another StopWords because there are only 179 english words in nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "other_stopwords = []\n",
    "with open(data_folder+'stopwords.txt', 'r') as file:\n",
    "    for st in file:\n",
    "        other_stopwords.append(st)\n",
    "\n",
    "other_stopwords = [x.replace('\\n', '') for x in other_stopwords]\n",
    "stopwords.extend(other_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference with nltk stopwords and the updated stopword\n",
    "#print(len(nltk.corpus.stopwords.words('english')))\n",
    "#print(len(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_file_dict = {}\n",
    "for line in data:\n",
    "    line = line.lower()\n",
    "    # here for tokenization i used nltk.word_tokenize\n",
    "    # TOKENIZATION\n",
    "    tokens = nltk.word_tokenize(line) \n",
    "    #print(tokens)\n",
    "    \n",
    "    # now Cleaning\n",
    "    clean_tokens = cleaning(tokens)\n",
    "    \n",
    "    #print(clean_tokens)\n",
    "    final_tok = []\n",
    "    for tok in clean_tokens:\n",
    "        tok = str(tok).rstrip(' ')\n",
    "        if(len(str(tok).split(' ')) > 1):\n",
    "            # TOKENIZATION\n",
    "            token = nltk.word_tokenize(tok)\n",
    "            cl_tok = cleaning(token)\n",
    "            for tk in cl_tok:\n",
    "                final_tok.append(tk)\n",
    "        else:\n",
    "            final_tok.append(tok)\n",
    "    \n",
    "    # Now Removing STOPWORDS\n",
    "    filtered_words = [w for w in final_tok if not w in stopwords]\n",
    "    \n",
    "    # To check the difference with the filtered words\n",
    "    #print(final_tok)\n",
    "    #print(filtered_words)\n",
    "   \n",
    "    # Now LEMMATIZATION\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    w_net_lem = []\n",
    "    for i in range(len(filtered_words)):\n",
    "        w_net_lem.append(lemmatizer.lemmatize(filtered_words[i]))\n",
    "     \n",
    "    # Now STEMMING over the Lemmatized word\n",
    "    po_stemmer = nltk.stem.PorterStemmer()\n",
    "    \n",
    "    #stemmed = [stem(w) for w in w_net_lem]\n",
    "    stemmed = [po_stemmer.stem(w) for w in w_net_lem]\n",
    "    \n",
    "    #print(w_net_lem)\n",
    "    #print(stemmed)\n",
    "    \n",
    "    # now save to file\n",
    "    # connect each word in the list of stem to make it value for the loaded file as index in dictiona\n",
    "    temp = ''\n",
    "    for i in stemmed:\n",
    "        temp = temp + i + ' '\n",
    "    \n",
    "    #print(temp)\n",
    "    cleaned_file_dict[line] = temp\n",
    "    \n",
    "    with open(data_folder+'cleaned.json', 'w') as f:\n",
    "        json.dump(cleaned_file_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "removing punctuation and unwanted character\n",
    "\n",
    "including from the list of tokens and inside the token itself\n",
    "\n",
    "e.g ['dr.', ',', 'son-in-law']\n",
    "\n",
    "removing ',' from the list\n",
    "\n",
    "changing 'dr.' into 'dr'\n",
    "\n",
    "changin 'son-in-law' into 'son in law'\n",
    "\n",
    "Note in Cleaning or Removing texts what we need to recall is\n",
    "\n",
    "The Preprocessing depends on the Task we are doing.\n",
    "\n",
    "so I assume that this preprocessing is for the Topic Analysis Task. or also for Search Engine\n",
    "\n",
    "e.g if its Machine Translation or Text to Speech there may be No words that we remove or clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# characters that are not found in string.punctuation\n",
    "punctuation = ['…','•','”','→','↑','“','‘','’','—','£','€','$']\n",
    "\n",
    "def cleaning(tokens):\n",
    "    cleaned_token = []\n",
    "    \n",
    "    # check for each token in each list of tokens(sentence or data)\n",
    "    for token in tokens:\n",
    "        # if the token has the punctuation.\n",
    "        if ((token_has_punct(token))):\n",
    "            # identify which character it is\n",
    "            chrs = find_punct(token) \n",
    "            new_token = token\n",
    "            for cr in chrs:\n",
    "                new_token = str(new_token.replace(cr,' ')) if cr == '-' else str(new_token).replace(cr, '')\n",
    "            \n",
    "            # if the token is number or not an english word pass over it else append to the list\n",
    "            if (is_float(new_token) or isNotEnglish(new_token)):\n",
    "                pass\n",
    "            else:\n",
    "                cleaned_token.append(new_token) \n",
    "        # if token has not punctuation and is not an english word pass over it\n",
    "        elif(isNotEnglish(token)):\n",
    "            pass\n",
    "        \n",
    "        # if token has not puncuation and is an english word append to the list\n",
    "        else:\n",
    "            cleaned_token.append(token)\n",
    "\n",
    "    return cleaned_token\n",
    "\n",
    "\n",
    "def isNotEnglish(token):\n",
    "    # check it by decoding the asii code of the character in token, \n",
    "    # if its no an english letter Error will raised\n",
    "    try:\n",
    "        token.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def token_has_punct(token):\n",
    "    # check each character in token\n",
    "    for cr in token: \n",
    "        if ((cr in string.punctuation) or cr in punctuation): return True \n",
    "        else: pass\n",
    "    return False\n",
    "\n",
    "def find_punct(token):\n",
    "    crs = []\n",
    "    for cr in token: crs.append(cr) if ((cr in string.punctuation) or (cr in punctuation)) else '' \n",
    "    return crs\n",
    "\n",
    "def is_float(x):\n",
    "    try: \n",
    "        float(x) \n",
    "        return True\n",
    "    except: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Lemmatization I used the Wordnet, which is a large, freely and publicly available lexical database for the English language\n",
    "\n",
    "and aiming to establish structured semantic relationships between words.\n",
    "\n",
    "It offers lemmatization capabilities as well is one of the earliest and most commonly used lemmatizer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK offers an interface to Wordnet but first we have to download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different algorithim for Stemming, like\n",
    "\n",
    "PorterStemmer in the NLTK\n",
    "\n",
    "Stemming \n",
    "\n",
    "so here I have used the PorterStemmer in the NLTK\n",
    "\n",
    "eventhough they have a little difference with Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
