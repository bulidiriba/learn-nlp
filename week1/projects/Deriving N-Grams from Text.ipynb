{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating N-grams From Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This used for Language Classification or Language Detection\n",
    "\n",
    "this is based on http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/\n",
    "\n",
    "what is n-grams see from http://cloudmark.github.io/Language-Detection/\n",
    "\n",
    "    N-gram means just an n-character slice of a longer string.\n",
    "typically one would slice the string into a set of overlapping N-grams.\n",
    "we can also append pads(blanks) to the beginning and end of strings in order to help with matching the beginning-of-word and end-of-word situations.\n",
    "\n",
    "e.g given the word 'TEXT', we can obtain the ff N-Grams\n",
    "\n",
    "bi-grams   _T, TE, EX, XT, T_\n",
    "\n",
    "tri-grams  _TE, TEX, EXT, XT_, T__\n",
    "\n",
    "quad-grams _TEX, TEXT, EXT_, XT__, T___\n",
    "\n",
    "How We can Detect Language with N-grams ?\n",
    "See the next Jupyter Books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Le temps est un grand maitre, dit-on, le malheur est qu 'il tue eleves.\"\n",
    "s = s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maitre, dit-on, le malheur est qu 'il tue eleves.\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'temps',\n",
       " 'est',\n",
       " 'un',\n",
       " 'grand',\n",
       " 'maitre',\n",
       " 'dit',\n",
       " 'on',\n",
       " 'le',\n",
       " 'malheur',\n",
       " 'est',\n",
       " 'qu',\n",
       " \"'il\",\n",
       " 'tue',\n",
       " 'eleves']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "s_tokenized = tokenizer.tokenize(s)\n",
    "s_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "generated_4grams = []\n",
    "for word in s_tokenized:\n",
    "    generated_4grams.append(list(ngrams(word, 4, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol=\"_\"))) # n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-2bd05ac15f20>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-2bd05ac15f20>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ngrams?\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ngrams\n",
    "# returns the ngrams generated from a sequence of items, as an iterator. \n",
    "# it tooks 6 parameters(sequence, n, pad_left=F, pad_right=T,left_pad_symbol=,right_pad_symbol)\n",
    "# n is number of n grams. you can take grams as characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('*', '*', '*', 'l'),\n",
       "  ('*', '*', 'l', 'e'),\n",
       "  ('*', 'l', 'e', '+'),\n",
       "  ('l', 'e', '+', '+'),\n",
       "  ('e', '+', '+', '+')],\n",
       " [('*', '*', '*', 't'),\n",
       "  ('*', '*', 't', 'e'),\n",
       "  ('*', 't', 'e', 'm'),\n",
       "  ('t', 'e', 'm', 'p'),\n",
       "  ('e', 'm', 'p', 's'),\n",
       "  ('m', 'p', 's', '+'),\n",
       "  ('p', 's', '+', '+'),\n",
       "  ('s', '+', '+', '+')],\n",
       " [('*', '*', '*', 'e'),\n",
       "  ('*', '*', 'e', 's'),\n",
       "  ('*', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '+'),\n",
       "  ('s', 't', '+', '+'),\n",
       "  ('t', '+', '+', '+')],\n",
       " [('*', '*', '*', 'u'),\n",
       "  ('*', '*', 'u', 'n'),\n",
       "  ('*', 'u', 'n', '+'),\n",
       "  ('u', 'n', '+', '+'),\n",
       "  ('n', '+', '+', '+')],\n",
       " [('*', '*', '*', 'g'),\n",
       "  ('*', '*', 'g', 'r'),\n",
       "  ('*', 'g', 'r', 'a'),\n",
       "  ('g', 'r', 'a', 'n'),\n",
       "  ('r', 'a', 'n', 'd'),\n",
       "  ('a', 'n', 'd', '+'),\n",
       "  ('n', 'd', '+', '+'),\n",
       "  ('d', '+', '+', '+')],\n",
       " [('*', '*', '*', 'm'),\n",
       "  ('*', '*', 'm', 'a'),\n",
       "  ('*', 'm', 'a', 'i'),\n",
       "  ('m', 'a', 'i', 't'),\n",
       "  ('a', 'i', 't', 'r'),\n",
       "  ('i', 't', 'r', 'e'),\n",
       "  ('t', 'r', 'e', '+'),\n",
       "  ('r', 'e', '+', '+'),\n",
       "  ('e', '+', '+', '+')],\n",
       " [('*', '*', '*', 'd'),\n",
       "  ('*', '*', 'd', 'i'),\n",
       "  ('*', 'd', 'i', 't'),\n",
       "  ('d', 'i', 't', '+'),\n",
       "  ('i', 't', '+', '+'),\n",
       "  ('t', '+', '+', '+')],\n",
       " [('*', '*', '*', 'o'),\n",
       "  ('*', '*', 'o', 'n'),\n",
       "  ('*', 'o', 'n', '+'),\n",
       "  ('o', 'n', '+', '+'),\n",
       "  ('n', '+', '+', '+')],\n",
       " [('*', '*', '*', 'l'),\n",
       "  ('*', '*', 'l', 'e'),\n",
       "  ('*', 'l', 'e', '+'),\n",
       "  ('l', 'e', '+', '+'),\n",
       "  ('e', '+', '+', '+')],\n",
       " [('*', '*', '*', 'm'),\n",
       "  ('*', '*', 'm', 'a'),\n",
       "  ('*', 'm', 'a', 'l'),\n",
       "  ('m', 'a', 'l', 'h'),\n",
       "  ('a', 'l', 'h', 'e'),\n",
       "  ('l', 'h', 'e', 'u'),\n",
       "  ('h', 'e', 'u', 'r'),\n",
       "  ('e', 'u', 'r', '+'),\n",
       "  ('u', 'r', '+', '+'),\n",
       "  ('r', '+', '+', '+')],\n",
       " [('*', '*', '*', 'e'),\n",
       "  ('*', '*', 'e', 's'),\n",
       "  ('*', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '+'),\n",
       "  ('s', 't', '+', '+'),\n",
       "  ('t', '+', '+', '+')],\n",
       " [('*', '*', '*', 'q'),\n",
       "  ('*', '*', 'q', 'u'),\n",
       "  ('*', 'q', 'u', '+'),\n",
       "  ('q', 'u', '+', '+'),\n",
       "  ('u', '+', '+', '+')],\n",
       " [('*', '*', '*', \"'\"),\n",
       "  ('*', '*', \"'\", 'i'),\n",
       "  ('*', \"'\", 'i', 'l'),\n",
       "  (\"'\", 'i', 'l', '+'),\n",
       "  ('i', 'l', '+', '+'),\n",
       "  ('l', '+', '+', '+')],\n",
       " [('*', '*', '*', 't'),\n",
       "  ('*', '*', 't', 'u'),\n",
       "  ('*', 't', 'u', 'e'),\n",
       "  ('t', 'u', 'e', '+'),\n",
       "  ('u', 'e', '+', '+'),\n",
       "  ('e', '+', '+', '+')],\n",
       " [('*', '*', '*', 'e'),\n",
       "  ('*', '*', 'e', 'l'),\n",
       "  ('*', 'e', 'l', 'e'),\n",
       "  ('e', 'l', 'e', 'v'),\n",
       "  ('l', 'e', 'v', 'e'),\n",
       "  ('e', 'v', 'e', 's'),\n",
       "  ('v', 'e', 's', '+'),\n",
       "  ('e', 's', '+', '+'),\n",
       "  ('s', '+', '+', '+')]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It seems that generated_4grams needs flattening since it's supposed to be a list of 4-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('*', '*', '*', 'l'),\n",
       " ('*', '*', 'l', 'e'),\n",
       " ('*', 'l', 'e', '+'),\n",
       " ('l', 'e', '+', '+'),\n",
       " ('e', '+', '+', '+'),\n",
       " ('*', '*', '*', 't'),\n",
       " ('*', '*', 't', 'e'),\n",
       " ('*', 't', 'e', 'm'),\n",
       " ('t', 'e', 'm', 'p'),\n",
       " ('e', 'm', 'p', 's')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_4grams = [word for sublist in generated_4grams for word in sublist]\n",
    "generated_4grams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Obtaining n-grams(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['***l',\n",
       " '**le',\n",
       " '*le+',\n",
       " 'le++',\n",
       " 'e+++',\n",
       " '***t',\n",
       " '**te',\n",
       " '*tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps+',\n",
       " 'ps++',\n",
       " 's+++',\n",
       " '***e',\n",
       " '**es',\n",
       " '*est',\n",
       " 'est+',\n",
       " 'st++',\n",
       " 't+++',\n",
       " '***u',\n",
       " '**un',\n",
       " '*un+',\n",
       " 'un++',\n",
       " 'n+++',\n",
       " '***g',\n",
       " '**gr',\n",
       " '*gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and+',\n",
       " 'nd++',\n",
       " 'd+++',\n",
       " '***m',\n",
       " '**ma',\n",
       " '*mai',\n",
       " 'mait',\n",
       " 'aitr',\n",
       " 'itre',\n",
       " 'tre+',\n",
       " 're++',\n",
       " 'e+++',\n",
       " '***d',\n",
       " '**di',\n",
       " '*dit',\n",
       " 'dit+',\n",
       " 'it++',\n",
       " 't+++',\n",
       " '***o',\n",
       " '**on',\n",
       " '*on+',\n",
       " 'on++',\n",
       " 'n+++',\n",
       " '***l',\n",
       " '**le',\n",
       " '*le+',\n",
       " 'le++',\n",
       " 'e+++',\n",
       " '***m',\n",
       " '**ma',\n",
       " '*mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur+',\n",
       " 'ur++',\n",
       " 'r+++',\n",
       " '***e',\n",
       " '**es',\n",
       " '*est',\n",
       " 'est+',\n",
       " 'st++',\n",
       " 't+++',\n",
       " '***q',\n",
       " '**qu',\n",
       " '*qu+',\n",
       " 'qu++',\n",
       " 'u+++',\n",
       " \"***'\",\n",
       " \"**'i\",\n",
       " \"*'il\",\n",
       " \"'il+\",\n",
       " 'il++',\n",
       " 'l+++',\n",
       " '***t',\n",
       " '**tu',\n",
       " '*tue',\n",
       " 'tue+',\n",
       " 'ue++',\n",
       " 'e+++',\n",
       " '***e',\n",
       " '**el',\n",
       " '*ele',\n",
       " 'elev',\n",
       " 'leve',\n",
       " 'eves',\n",
       " 'ves+',\n",
       " 'es++',\n",
       " 's+++']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is just joining the scattered pieces of grams above together\n",
    "\n",
    "ng_list_4grams = generated_4grams\n",
    "for idx, val in enumerate(generated_4grams):\n",
    "    #print(idx, \"\\t\", val)\n",
    "    ng_list_4grams[idx] = ''.join(val)\n",
    "ng_list_4grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sorting n-grams by frequency (n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_4grams = {}\n",
    "for ngram in ng_list_4grams:\n",
    "    if ngram not in freq_4grams:\n",
    "        freq_4grams.update({ngram: 1})\n",
    "    else:\n",
    "        ngram_occurrences = freq_4grams[ngram]\n",
    "        freq_4grams.update({ngram: ngram_occurrences + 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator as op # The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python.\n",
    "# e.g operator.add(x,y) is equivalen to expression x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_4grams_sorted = sorted(freq_4grams.items(), key=op.itemgetter(1), reverse=True)[0:300]\n",
    "# we only keep the 300 most popular n grams. this was suggested in the original paper written about n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(freq_4grams_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Obtaining n-grams by frequency (n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get n-grams for n=1, 2, 3 and 4 we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maitre dit on le malheur est qu 'il tue eleves\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import everygrams\n",
    "\n",
    "s_clean = ' '.join(s_tokenized) # for the code below we need the raw sentence as opposed to the tokens\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_extractor(sent):\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4)\n",
    "           if ' ' not in ng and '\\n' not in ng and ng != ('_',)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'm',\n",
       " 'p',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'n',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " 'm',\n",
       " 'a',\n",
       " 'i',\n",
       " 't',\n",
       " 'r',\n",
       " 'e',\n",
       " 'd',\n",
       " 'i',\n",
       " 't',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'e',\n",
       " 'm',\n",
       " 'a',\n",
       " 'l',\n",
       " 'h',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'q',\n",
       " 'u',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'l',\n",
       " 't',\n",
       " 'u',\n",
       " 'e',\n",
       " 'e',\n",
       " 'l',\n",
       " 'e',\n",
       " 'v',\n",
       " 'e',\n",
       " 's',\n",
       " 'le',\n",
       " 'e_',\n",
       " '_t',\n",
       " 'te',\n",
       " 'em',\n",
       " 'mp',\n",
       " 'ps',\n",
       " 's_',\n",
       " '_e',\n",
       " 'es',\n",
       " 'st',\n",
       " 't_',\n",
       " '_u',\n",
       " 'un',\n",
       " 'n_',\n",
       " '_g',\n",
       " 'gr',\n",
       " 'ra',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'd_',\n",
       " '_m',\n",
       " 'ma',\n",
       " 'ai',\n",
       " 'it',\n",
       " 'tr',\n",
       " 're',\n",
       " 'e_',\n",
       " '_d',\n",
       " 'di',\n",
       " 'it',\n",
       " 't_',\n",
       " '_o',\n",
       " 'on',\n",
       " 'n_',\n",
       " '_l',\n",
       " 'le',\n",
       " 'e_',\n",
       " '_m',\n",
       " 'ma',\n",
       " 'al',\n",
       " 'lh',\n",
       " 'he',\n",
       " 'eu',\n",
       " 'ur',\n",
       " 'r_',\n",
       " '_e',\n",
       " 'es',\n",
       " 'st',\n",
       " 't_',\n",
       " '_q',\n",
       " 'qu',\n",
       " 'u_',\n",
       " \"_'\",\n",
       " \"'i\",\n",
       " 'il',\n",
       " 'l_',\n",
       " '_t',\n",
       " 'tu',\n",
       " 'ue',\n",
       " 'e_',\n",
       " '_e',\n",
       " 'el',\n",
       " 'le',\n",
       " 'ev',\n",
       " 've',\n",
       " 'es',\n",
       " 'le_',\n",
       " '_te',\n",
       " 'tem',\n",
       " 'emp',\n",
       " 'mps',\n",
       " 'ps_',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " '_un',\n",
       " 'un_',\n",
       " '_gr',\n",
       " 'gra',\n",
       " 'ran',\n",
       " 'and',\n",
       " 'nd_',\n",
       " '_ma',\n",
       " 'mai',\n",
       " 'ait',\n",
       " 'itr',\n",
       " 'tre',\n",
       " 're_',\n",
       " '_di',\n",
       " 'dit',\n",
       " 'it_',\n",
       " '_on',\n",
       " 'on_',\n",
       " '_le',\n",
       " 'le_',\n",
       " '_ma',\n",
       " 'mal',\n",
       " 'alh',\n",
       " 'lhe',\n",
       " 'heu',\n",
       " 'eur',\n",
       " 'ur_',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " '_qu',\n",
       " 'qu_',\n",
       " \"_'i\",\n",
       " \"'il\",\n",
       " 'il_',\n",
       " '_tu',\n",
       " 'tue',\n",
       " 'ue_',\n",
       " '_el',\n",
       " 'ele',\n",
       " 'lev',\n",
       " 'eve',\n",
       " 'ves',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps_',\n",
       " '_est',\n",
       " 'est_',\n",
       " '_un_',\n",
       " '_gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and_',\n",
       " '_mai',\n",
       " 'mait',\n",
       " 'aitr',\n",
       " 'itre',\n",
       " 'tre_',\n",
       " '_dit',\n",
       " 'dit_',\n",
       " '_on_',\n",
       " '_le_',\n",
       " '_mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur_',\n",
       " '_est',\n",
       " 'est_',\n",
       " '_qu_',\n",
       " \"_'il\",\n",
       " \"'il_\",\n",
       " '_tue',\n",
       " 'tue_',\n",
       " '_ele',\n",
       " 'elev',\n",
       " 'leve',\n",
       " 'eves']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_extractor(s_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
